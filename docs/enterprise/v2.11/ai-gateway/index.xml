<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Gateway - API Gateway for LLMs on KrakenD - Open source API Gateway</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/</link><description>Recent content in AI Gateway - API Gateway for LLMs on KrakenD - Open source API Gateway</description><generator>Hugo</generator><language>en</language><lastBuildDate>Wed, 17 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/index.xml" rel="self" type="application/rss+xml"/><item><title>AI Security</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/security/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/security/</guid><description>Protecting sensitive AI data and controlling access is essential for trustworthy AI workloads. KrakenD&amp;rsquo;s AI Gateway integrates multiple layers of security at the edge to enforce zero-trust AI operations. From isolated authorization flows to data masking and exfiltration prevention, you can safeguard data traveling between clients and LLM providers without altering your existing API infrastructure.
The following key features are explained in this document:
Isolated Authentication (JWT + API key separation) API Key Injection (backend-only exposure) Data Masking (request &amp;amp; response layers) Exfiltration Prevention Patterns Isolated Authentication KrakenD separates the authentication flows between consumers and LLM on your AI endpoints.</description></item><item><title>AI Token Cost Control &amp; Quotas</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/budget-control/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/budget-control/</guid><description>AI workloads can quickly generate unpredictable and excessive costs. KrakenD&amp;rsquo;s AI Gateway provides granular token usage monitoring and enforcement to keep your AI expenses transparent and within budget. Features like token quotas, budget alerts, prompt caching, and intelligent routing enable you to optimize requests and avoid surprise bills while maintaining performance and scalability.
Token Quota and Budget Enforcement KrakenD Enterprise includes a powerful persistent quota system that&amp;rsquo;s perfect for managing token-based usage quotas in LLM applications, designer for controlling cost, enforcing subscription tiers, and preventing overuse.</description></item><item><title>AI Governance</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/governance/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/governance/</guid><description>KrakenD AI Governance empowers organizations to deploy large language models responsibly by enforcing compliance, security, and operational guardrails inline with AI traffic. Leverage granular controls on prompts, responses, usage, and reuse of prompt templates to standardize AI across teams, tenants, and projects.
Prompt Policy Enforcement Prompt policies enforce constraints on input prompts by pattern matching, contextual validation, or checking request metadata to prevent abusive or sensitive content from being processed downstream.</description></item><item><title>Unified LLM Interface</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/unified-llm-interface/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/unified-llm-interface/</guid><description>The Unified LLM Interface of KrakenD allows you to interact with one or more LLMs, removing the complexity of dealing with LLMs for the end user. It allows you to set the grounds to communicate with LLMs, authenticate, and treat requests and responses back and forth.
The unified LLM interface helps you to:
Route to the right LLM Abstract the request interface Abstract the response interface Routing to the right LLM KrakenD&amp;rsquo;s LLM Routing and AI Proxy feature enables the distribution of AI requests across one or multiple Large Language Model.</description></item><item><title>LLM Routing</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/llm-routing/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/llm-routing/</guid><description>KrakenD’s AI Proxy and LLM routing feature enables you to distribute AI requests across one or multiple Large Language Model providers or instances.
LLM Routing on KrakenD supports both single-provider and multi-provider. You can configure endpoints that can connect to a specific LLM model, or based on policies, change the model on the fly, or they even make simultaneous requests to different providers to aggregate their responses.
To implement single or multi-LLM routing in KrakenD, like dynamically selecting between different Large Language Model (LLM) providers like OpenAI, Mistral, Anthropic, or custom models, there are several clean, scalable strategies depending on how you want to choose the provider:</description></item><item><title>OpenAI Integration</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/openai/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/openai/</guid><description>The OpenAI interface allows KrakenD to use OpenAI&amp;rsquo;s API without writing custom integration code, enabling intelligent automation, content generation, or any LLM-powered use case within your existing API infrastructure.
This component abstracts you from the OpenAI API usage allowing the consumer to concentrate on the prompt only, as for each request to an endpoint, KrakenD will create the OpenAI request with all the necessary elements in their API, and will return a unified response, so if you use other vendors you have a consitent use of LLM models.</description></item><item><title>Google Gemini integration</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/gemini/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/gemini/</guid><description>The Gemini interface allows KrakenD to use Gemini&amp;rsquo;s API without writing custom integration code, enabling intelligent automation, content generation, or any LLM-powered use case within your existing API infrastructure.
This component abstracts you from the Gemini API usage allowing the consumer to concentrate on the prompt only, as for each request to an endpoint, KrakenD will create the Gemini request with all the necessary elements in their API, and will return a unified response, so if you use other vendors you have a consitent use of LLM models.</description></item><item><title>Mistral Integration</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/mistral/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/mistral/</guid><description>The Mistral integration enables KrakenD to talk to Mistral’s AI services, allowing you to embed powerful language model capabilities into your API workflows without custom coding. Use this interface when you want to connect Mistral for tasks like intelligent automation, conversational AI, content generation, or other LLM-driven functionalities inside your existing API infrastructure.
This component abstracts the complexities of communicating with Mistral’s AI API. When an API request hits your KrakenD endpoint configured with the Mistral interface, KrakenD automatically constructs the necessary Mistral API payload, handles authentication, and processes the responses uniformly.</description></item><item><title>Anthropic integration</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/anthropic/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/anthropic/</guid><description>The Anthropic interface allows KrakenD to use Anthropic&amp;rsquo;s API (Claude) without writing custom integration code, enabling intelligent automation, content generation, or any LLM-powered use case within your existing API infrastructure.
This component abstracts you from the Anthropic API usage allowing the consumer to concentrate on the prompt only, as for each request to an endpoint, KrakenD will create the Anthropic request with all the necessary elements in their API, and will return a unified response, so if you use other vendors you have a consitent use of LLM models.</description></item><item><title>Connecting to other AI vendors</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/other-vendors/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/other-vendors/</guid><description>If you need to connect to an AI vendor that is not listed in the pre-defined list of abstracted interfaces, you can still perform the abstraction by providing a template.
On KrakenD you can implement completely different models while you keep the user away from this process.
The way to adapt the payload sent to each LLM, you need to pass the right request template to each LLM. This is achieved with the request body generator which takes care of this job.</description></item><item><title>AI usage monitoring</title><link>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/usage-monitoring/</link><pubDate>Wed, 17 Sep 2025 00:00:00 +0000</pubDate><guid>https://www.krakend.io/docs/enterprise/v2.11/ai-gateway/usage-monitoring/</guid><description>Monitoring AI backends in KrakenD does not require special treatment. AI services are integrated and handled just like any other backend in your API Gateway. Your existing monitoring tools and practices, such as OpenTelemetry, remain fully applicable.
AI backends, regardless of their complexity, respond to API calls just like any other backend service. KrakenD treats AI integrations as regular backends. This simplifies observability and operational consistency by applying proven monitoring frameworks without specialized or separate tooling.</description></item></channel></rss>